procgen-rainbow:
    env: procgen_env_wrapper
    run: DQN
    stop:
        timesteps_total: 4000000
    checkpoint_freq: 100
    checkpoint_at_end: True
    keep_checkpoints_num: 5
    config:
        env_config:
            # Name of the procgen environment to train on # Note, that this parameter will be overriden during the evaluation by the AIcrowd evaluators.
            env_name: coinrun
            # The number of unique levels that can be generated. Set to 0 to use unlimited levels
            num_levels: 0
            # The lowest seed that will be used to generated levels. 'start_level' and 'num_levels' fully specify the set of possible levels
            start_level: 0
            # Paint player velocity info in the top left corner. Only supported by certain games.
            paint_vel_info: False
            # Use randomly generated assets in place of human designed assets
            use_generated_assets: False
            # center_agent : Determines whether observations are centered on the agent or display the full level. Override at your own risk.
            center_agent: True
            # sequential levels : When you reach the end of a level, the episode is ended and a new level is selected. If use_sequential_levels is set to True, reaching the end of a level does not end the episode, and the seed for the new level is derived from the current level seed. If you combine this with start_level=<some seed> and num_levels=1, you can have a single linear series of levels similar to a gym-retro or ALE game.
            use_sequential_levels: False
            # What variant of the levels to use, the options are "easy", "hard", "extreme", "memory", "exploration". All games support "easy" and "hard", while other options are game-specific. The default is "hard". Switching to "easy" will reduce the number of timesteps required to solve each game and is useful for testing or when working with limited compute resources. NOTE : During the evaluation phase (rollout), this will always be overriden to "easy"
            distribution_mode: easy
        num_atoms: 51
        v_min: -10.0
        v_max: 10.0
        noisy: True
        dueling: True
        double_q: True
        gamma: 0.999
        lr: .0005
        grad_clip: 40
        #hiddens: [512]
        learning_starts: 1000
        buffer_size: 2000
        rollout_fragment_length: 4
        train_batch_size: 32
        
        target_network_update_freq: 500
        
        prioritized_replay: True
        prioritized_replay_alpha: 0.5
        final_prioritized_replay_beta: 1.0
        prioritized_replay_beta_annealing_timesteps: 400000
        
        n_step: 3
        num_gpus: 0.3
        num_workers: 1
        use_pytorch: False
        num_cpus_per_worker: 1
        num_gpus_per_worker: 0.1
        num_cpus_for_driver: 1
        preprocessor_pref: deepmind
        # Element-wise observation filter, either "NoFilter" or "MeanStdFilter".
        observation_filter: "NoFilter"
        # Whether to synchronize the statistics of remote filters.
        synchronize_filters: True
        # Whether to LZ4 compress individual observations
        compress_observations: False
        # Minimum env steps to optimize for per train call. This value does
        # not affect learning, only the length of train iterations.
        timesteps_per_iteration: 0
        seed: null
        ignore_worker_failures: False
        # Log system resource metrics to results. This requires `psutil` to be
        # installed for sys stats, and `gputil` for GPU metrics.
        # Note : The AIcrowd Evaluators will always override this to be True
        log_sys_usage: True
        explore: True
        # Provide a dict specifying the Exploration object's config.
        exploration_config:
            # The Exploration class to use. In the simplest case, this is the name
            # (str) of any class present in the `rllib.utils.exploration` package.
            # You can also provide the python class directly or the full location
            # of your class (e.g. "ray.rllib.utils.exploration.epsilon_greedy.
            # EpsilonGreedy)
            type: "EpsilonGreedy"
            # Can add constructor kwargs here (if any)
            initial_epsilon: 1.0
            final_epsilon: 0.02
            epsilon_timesteps: 1000000
        model:
          conv_filters: [[16, [4, 4], 2], [32, [4, 4], 2], [512, [16, 16], 1]]
          grayscale: True
          zero_mean: False
          dim: 64