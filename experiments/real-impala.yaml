procgen-impala:
    env: procgen_env_wrapper  # Change this at your own risk :D
    run: IMPALA
    
    stop:
        timesteps_total: 8000000

    
    checkpoint_freq: 100
    checkpoint_at_end: True
    keep_checkpoints_num: 5

    config:
        env_config:
            # Name of the procgen environment to train on # Note, that this parameter will be overriden during the evaluation by the AIcrowd evaluators.
            env_name: coinrun
            # The number of unique levels that can be generated. Set to 0 to use unlimited levels
            num_levels: 0
            # The lowest seed that will be used to generated levels. 'start_level' and 'num_levels' fully specify the set of possible levels
            start_level: 0
            # Paint player velocity info in the top left corner. Only supported by certain games.
            paint_vel_info: True
            # Use randomly generated assets in place of human designed assets
            use_generated_assets: False
            # center_agent : Determines whether observations are centered on the agent or display the full level. Override at your own risk.
            center_agent: True
            # sequential levels : When you reach the end of a level, the episode is ended and a new level is selected. If use_sequential_levels is set to True, reaching the end of a level does not end the episode, and the seed for the new level is derived from the current level seed. If you combine this with start_level=<some seed> and num_levels=1, you can have a single linear series of levels similar to a gym-retro or ALE game.
            use_sequential_levels: False
            # What variant of the levels to use, the options are "easy", "hard", "extreme", "memory", "exploration". All games support "easy" and "hard", while other options are game-specific. The default is "hard". Switching to "easy" will reduce the number of timesteps required to solve each game and is useful for testing or when working with limited compute resources. NOTE : During the evaluation phase (rollout), this will always be overriden to "easy"
            distribution_mode: easy
            use_backgrounds: False
        
        gamma: 0.999
        lr: 0.001
        num_sgd_iter: 2
        train_batch_size: 2048
        grad_clip: 1
        clip_rewards: null
        opt_type: adam
        lr_schedule: null
        decay: 0.99
        momentum: 0.0
        epsilon: 0.1
        # balancing the three losses
        vf_loss_coeff: 1.9572
        entropy_coeff: 0.00111
        entropy_coeff_schedule: null
        
        model:
            custom_model: impala_cnn_tf
            custom_options: {}
            
        # V-trace params (see vtrace_tf/torch.py).
        vtrace: True
        vtrace_clip_rho_threshold: 1.0
        vtrace_clip_pg_rho_threshold: 1.0
        
        rollout_fragment_length: 50
        
        
        
        
        min_iter_time_s: 10
        # set >1 to load data into GPUs in parallel. Increases GPU memory usage
        # proportionally with the number of buffers.
        num_data_loader_buffers: 1
        # how many train batches should be retained for minibatching. This conf
        # only has an effect if `num_sgd_iter > 1`.
        minibatch_buffer_size: 1
        # set >0 to enable experience replay. Saved samples will be replayed with
        # a p:1 proportion to new data samples.
        replay_proportion: 0.0
        # number of sample batches to store for replay. The number of transitions
        # saved total will be (replay_buffer_num_slots * rollout_fragment_length).
        replay_buffer_num_slots: 0
        # max queue size for train batches feeding into the learner
        learner_queue_size: 16
        # wait for train batches to be available in minibatch buffer queue
        # this many seconds. This may need to be increased e.g. when training
        # with a slow environment
        learner_queue_timeout: 3000
        # level of queuing for sampling.
        max_sample_requests_in_flight_per_worker: 2
        # max number of workers to broadcast one set of weights to
        broadcast_interval: 1
        # use intermediate actors for multi-level aggregation. This can make sense
        # if ingesting >2GB/s of samples, or if the data requires decompression.
        num_aggregation_workers: 0

        
        
        
        
        # Which observation filter to apply to the observation.
        observation_filter: NoFilter
        # Number of steps after which the episode is forced to terminate. Defaults
        # to `env.spec.max_episode_steps` (if present) for Gym envs.
        horizon: null
        # Calculate rewards but don't reset the environment when the horizon is
        # hit. This allows value estimation and RNN state to span across logical
        # episodes denoted by horizon. This only has an effect if horizon != inf.
        soft_horizon: False
        # Don't set 'done' at the end of the episode. Note that you still need to
        # set this if soft_horizon=True, unless your env is actually running
        # forever without returning done=True.
        no_done_at_end: False

        # Unsquash actions to the upper and lower bounds of env's action space
        normalize_actions: False
        
        # Whether to np.clip() actions to the action space low/high range spec.
        clip_actions: True
        # Whether to use rllib or deepmind preprocessors by default
        preprocessor_pref: deepmind

        # Whether to attempt to continue training if a worker crashes. The number
        # of currently healthy workers is reported as the "num_healthy_workers"
        # metric.
        ignore_worker_failures: False
        # Log system resource metrics to results. This requires `psutil` to be
        # installed for sys stats, and `gputil` for GPU metrics.
        # Note : The AIcrowd Evaluators will always override this to be True
        log_sys_usage: True

        # Use PyTorch (instead of tf). If using `rllib train`, this can also be
        # enabled with the `--torch` flag.
        # NOTE: Some agents may not support `torch` yet and throw an error.
        use_pytorch: False

        ################################################
        ################################################
        # === Settings for Rollout Worker processes ===
        ################################################
        ################################################
        # Number of rollout worker actors to create for parallel sampling. Setting
        # this to 0 will force rollouts to be done in the trainer actor.
        num_workers: 6

        num_envs_per_worker: 12
        batch_mode: truncate_episodes

        ################################################
        ################################################
        # === Advanced Resource Settings ===
        ################################################
        ################################################
        # Number of CPUs to allocate per worker.
        num_cpus_per_worker: 1
        # Number of GPUs to allocate per worker. This can be fractional. This is
        # usually needed only if your env itself requires a GPU (i.e., it is a
        # GPU-intensive video game), or model inference is unusually expensive.
        num_gpus_per_worker: 0.1
        # Number of CPUs to allocate for the trainer. Note: this only takes effect
        # when running in Tune. Otherwise, the trainer runs in the main program.
        num_cpus_for_driver: 1

        ################################################
        ################################################
        # === Settings for the Trainer process ===
        ################################################
        ################################################
        # Number of GPUs to allocate to the trainer process. Note that not all
        # algorithms can take advantage of trainer GPUs. This can be fractional
        # (e.g., 0.3 GPUs).
        # Note : If GPUs are not available, this will be overriden by the AIcrowd evaluators to 0.
        num_gpus: 0.3

        ################################################
        ################################################
        # === Exploration Settings ===
        ################################################
        ################################################
        # Default exploration behavior, iff `explore`=None is passed into
        # compute_action(s).
        # Set to False for no exploration behavior (e.g., for evaluation).
        explore: True
        # Provide a dict specifying the Exploration object's config.
        exploration_config:
            # The Exploration class to use. In the simplest case, this is the name
            # (str) of any class present in the `rllib.utils.exploration` package.
            # You can also provide the python class directly or the full location
            # of your class (e.g. "ray.rllib.utils.exploration.epsilon_greedy.
            # EpsilonGreedy)
            type: "StochasticSampling"
            # Can add constructor kwargs here (if any)
        
        evaluation_config:
            explore: False


        ################################################
        ################################################
        # === Advanced Rollout Settings ===
        ################################################
        ################################################
        # Element-wise observation filter, either "NoFilter" or "MeanStdFilter".
        observation_filter: "NoFilter"
        # Whether to synchronize the statistics of remote filters.
        synchronize_filters: True
        # Whether to LZ4 compress individual observations
        compress_observations: False
        # Minimum env steps to optimize for per train call. This value does
        # not affect learning, only the length of train iterations.
        timesteps_per_iteration: 0
        # This argument, in conjunction with worker_index, sets the random seed of
        # each worker, so that identically configured trials will have identical
        # results. This makes experiments reproducible.
        seed: null
        
